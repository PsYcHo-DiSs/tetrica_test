import requests
from bs4 import BeautifulSoup
from collections import defaultdict
import pandas as pd
import time
from urllib3.exceptions import SSLError
from requests.exceptions import RequestException

BASE_URL = "https://ru.wikipedia.org"
CATEGORY_URL = f"{BASE_URL}/wiki/–ö–∞—Ç–µ–≥–æ—Ä–∏—è:–ñ–∏–≤–æ—Ç–Ω—ã–µ_–ø–æ_–∞–ª—Ñ–∞–≤–∏—Ç—É"
OUTPUT_CSV = "beasts_bs4_.csv"


def fetch_html(url, retries=3, delay=1.0):
    """–ü–æ–ª—É—á–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∑–∞–¥–∞–Ω–Ω–æ–≥–æ URL —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
    for attempt in range(1, retries + 1):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except (SSLError, RequestException) as e:
            print(f"[{attempt}/{retries}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}:\n{e}")
            if attempt < retries:
                time.sleep(delay)
            else:
                raise


def parse_animals_from_page(html):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–∞ –∂–∏–≤–æ—Ç–Ω—ã—Ö —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (—Å–ª–æ–≤–∞—Ä—å {–±—É–∫–≤–∞: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ}, —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–ª–∏ None).
    """
    soup = BeautifulSoup(html, 'html.parser')
    counts = defaultdict(int)

    for link in soup.select('#mw-pages li a'):
        name = link.text.strip()
        if name:
            first_letter = name[0].upper()
            counts[first_letter] += 1

    next_link = soup.select_one('#mw-pages a:-soup-contains("–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")')
    next_url = BASE_URL + next_link['href'] if next_link else None

    return counts, next_url


def collect_all_counts(start_url):
    """–ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
    total_counts = defaultdict(int)
    current_url = start_url
    page_counter = 0

    while current_url:
        page_counter += 1
        print(f"‚Üí –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_counter}: {current_url}")
        try:
            html = fetch_html(current_url)
            page_counts, current_url = parse_animals_from_page(html)
            for letter, count in page_counts.items():
                total_counts[letter] += count
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_url}: {e}")
            break

        if page_counter % 10 == 0:
            print(f"‚úì –ü—Ä–æ–π–¥–µ–Ω–æ {page_counter} —Å—Ç—Ä–∞–Ω–∏—Ü")

        time.sleep(0.1)

    return total_counts


def save_counts_to_csv(counts, filename):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ CSV."""
    df = pd.DataFrame(sorted(counts.items()), columns=['–ë—É–∫–≤–∞', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'])
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")


if __name__ == '__main__':
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö...")
    final_counts = collect_all_counts(CATEGORY_URL)
    save_counts_to_csv(final_counts, OUTPUT_CSV)
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")
